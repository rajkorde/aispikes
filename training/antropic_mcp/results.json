{
  "topic": "Advances in LLM Training",
  "papers": [
    {
      "id": "2401.03129v1",
      "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
      "authors": ["Chen-An Li", "Hung-Yi Lee"],
      "published": "2024-01-06",
      "summary": "This paper investigates the problem of catastrophic forgetting that occurs during continual pre-training of fine-tuned LLMs. The researchers evaluate how continuous pre-training affects various aspects of fine-tuned LLMs, including output format, knowledge retention, and reliability. Their experiments highlight the significant challenge of addressing catastrophic forgetting, particularly regarding repetition issues in model outputs. This research is important for understanding how to effectively update LLMs without losing previously learned capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2401.03129v1"
    },
    {
      "id": "2401.13601v5",
      "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
      "authors": ["Duzhen Zhang", "Yahan Yu", "Jiahua Dong", "Chenxing Li", "Dan Su", "Chenhui Chu", "Dong Yu"],
      "published": "2024-01-24",
      "summary": "This comprehensive survey examines the substantial advancements in MultiModal Large Language Models (MM-LLMs) that augment traditional LLMs to support multimodal inputs and outputs through cost-effective training strategies. The paper outlines general design formulations for model architecture and training pipelines, introduces a taxonomy of 126 MM-LLMs, reviews performance on mainstream benchmarks, and summarizes key training techniques. The authors also explore promising future directions for MM-LLMs while maintaining a real-time tracking website for the latest developments in the field.",
      "pdf_url": "http://arxiv.org/pdf/2401.13601v5"
    },
    {
      "id": "2404.14809v1",
      "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
      "authors": ["Wenbo Shang", "Xin Huang"],
      "published": "2024-04-23",
      "summary": "This survey investigates how LLMs can be applied to graph analytics tasks, categorizing the work into LLM-based graph query processing (LLM-GQP) and LLM-based graph inference and learning (LLM-GIL). The paper explores how LLMs can generalize graph tasks without requiring specialized graph learning models or manual annotations. It summarizes useful prompting techniques for different graph tasks, evaluates model performance, discusses benchmark datasets, and analyzes the strengths and weaknesses of LLM approaches to graph analytics. The authors also identify open problems and future research directions in this interdisciplinary area.",
      "pdf_url": "http://arxiv.org/pdf/2404.14809v1"
    },
    {
      "id": "2407.12036v2",
      "title": "Exploring Advanced Large Language Models with LLMsuite",
      "authors": ["Giorgio Roffo"],
      "published": "2024-07-01",
      "summary": "This tutorial paper explores advancements and challenges in LLM development, addressing inherent limitations like knowledge cutoffs and mathematical inaccuracies. It proposes solutions including Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks like ReAct and LangChain. The paper covers fine-tuning strategies including instruction fine-tuning, parameter-efficient methods like LoRA, Reinforcement Learning from Human Feedback (RLHF), and Reinforced Self-Training (ReST). It also provides a comprehensive survey of transformer architectures and training techniques for LLMs, offering practical insights into improving LLM performance and reliability.",
      "pdf_url": "http://arxiv.org/pdf/2407.12036v2"
    },
    {
      "id": "2502.11275v1",
      "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
      "authors": ["Letian Peng", "Zilong Wang", "Feng Yao", "Jingbo Shang"],
      "published": "2025-02-16",
      "summary": "This paper introduces 'Cuckoo,' an innovative information extraction (IE) model that leverages the massive data resources prepared for LLMs. The researchers reframe next-token prediction as extraction for tokens already present in the context through a next tokens extraction (NTE) paradigm. This approach allows Cuckoo to learn from 102.6M extractive data converted from LLM's pre-training and post-training data. Under few-shot settings, Cuckoo adapts effectively to traditional and complex instruction-following IE tasks, outperforming existing pre-trained IE models while naturally evolving with ongoing advancements in LLM data preparation.",
      "pdf_url": "http://arxiv.org/pdf/2502.11275v1"
    }
  ]
}