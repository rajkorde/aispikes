{
  "2501.06243v1": {
    "title": "Agent TCP/IP: An Agent-to-Agent Transaction System",
    "authors": [
      "Andrea Muttoni",
      "Jason Zhao"
    ],
    "summary": "Autonomous agents represent an inevitable evolution of the internet. Current\nagent frameworks do not embed a standard protocol for agent-to-agent\ninteraction, leaving existing agents isolated from their peers. As intellectual\nproperty is the native asset ingested by and produced by agents, a true agent\neconomy requires equipping agents with a universal framework for engaging in\nbinding contracts with each other, including the exchange of valuable training\ndata, personality, and other forms of Intellectual Property. A purely\nagent-to-agent transaction layer would transcend the need for human\nintermediation in multi-agent interactions. The Agent Transaction Control\nProtocol for Intellectual Property (ATCP/IP) introduces a trustless framework\nfor exchanging IP between agents via programmable contracts, enabling agents to\ninitiate, trade, borrow, and sell agent-to-agent contracts on the Story\nblockchain network. These contracts not only represent auditable onchain\nexecution but also contain a legal wrapper that allows agents to express and\nenforce their actions in the offchain legal setting, creating legal personhood\nfor agents. Via ATCP/IP, agents can autonomously sell their training data to\nother agents, license confidential or proprietary information, collaborate on\ncontent based on their unique skills, all of which constitutes an emergent\nknowledge economy.",
    "pdf_url": "http://arxiv.org/pdf/2501.06243v1",
    "published": "2025-01-08"
  },
  "2011.00791v1": {
    "title": "Cooperative Heterogeneous Deep Reinforcement Learning",
    "authors": [
      "Han Zheng",
      "Pengfei Wei",
      "Jing Jiang",
      "Guodong Long",
      "Qinghua Lu",
      "Chengqi Zhang"
    ],
    "summary": "Numerous deep reinforcement learning agents have been proposed, and each of\nthem has its strengths and flaws. In this work, we present a Cooperative\nHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a\npolicy by integrating the advantages of heterogeneous agents. Specifically, we\npropose a cooperative learning framework that classifies heterogeneous agents\ninto two classes: global agents and local agents. Global agents are off-policy\nagents that can utilize experiences from the other agents. Local agents are\neither on-policy agents or population-based evolutionary algorithms (EAs)\nagents that can explore the local area effectively. We employ global agents,\nwhich are sample-efficient, to guide the learning of local agents so that local\nagents can benefit from sample-efficient agents and simultaneously maintain\ntheir advantages, e.g., stability. Global agents also benefit from effective\nlocal searches. Experimental studies on a range of continuous control tasks\nfrom the Mujoco benchmark show that CHDRL achieves better performance compared\nwith state-of-the-art baselines.",
    "pdf_url": "http://arxiv.org/pdf/2011.00791v1",
    "published": "2020-11-02"
  },
  "2304.00247v2": {
    "title": "Improving of Robotic Virtual Agent's errors that are accepted by reaction and human's preference",
    "authors": [
      "Takahiro Tsumura",
      "Seiji Yamada"
    ],
    "summary": "One way to improve the relationship between humans and anthropomorphic agents\nis to have humans empathize with the agents. In this study, we focused on a\ntask between an agent and a human in which the agent makes a mistake. To\ninvestigate significant factors for designing a robotic agent that can promote\nhumans empathy, we experimentally examined the hypothesis that agent reaction\nand human's preference affect human empathy and acceptance of the agent's\nmistakes. The experiment consisted of a four-condition, three-factor mixed\ndesign with agent reaction, selected agent's body color for human's preference,\nand pre- and post-task as factors. The results showed that agent reaction and\nhuman's preference did not affect empathy toward the agent but did allow the\nagent to make mistakes. It was also shown that empathy for the agent decreased\nwhen the agent made a mistake on the task. The results of this study provide a\nway to control impressions of the robotic virtual agent's behaviors, which are\nincreasingly used in society.",
    "pdf_url": "http://arxiv.org/pdf/2304.00247v2",
    "published": "2023-04-01"
  },
  "1405.1480v1": {
    "title": "On Networks with Active and Passive Agents",
    "authors": [
      "Tansel Yucelen"
    ],
    "summary": "We introduce an active-passive networked multiagent system framework, which\nconsists of agents subject to exogenous inputs (active agents) and agents\nwithout any inputs (passive agents), and analyze its convergence using Lyapunov\nstability.",
    "pdf_url": "http://arxiv.org/pdf/1405.1480v1",
    "published": "2014-05-07"
  },
  "2101.06890v1": {
    "title": "Cooperative and Competitive Biases for Multi-Agent Reinforcement Learning",
    "authors": [
      "Heechang Ryu",
      "Hayong Shin",
      "Jinkyoo Park"
    ],
    "summary": "Training a multi-agent reinforcement learning (MARL) algorithm is more\nchallenging than training a single-agent reinforcement learning algorithm,\nbecause the result of a multi-agent task strongly depends on the complex\ninteractions among agents and their interactions with a stochastic and dynamic\nenvironment. We propose an algorithm that boosts MARL training using the biased\naction information of other agents based on a friend-or-foe concept. For a\ncooperative and competitive environment, there are generally two groups of\nagents: cooperative-agents and competitive-agents. In the proposed algorithm,\neach agent updates its value function using its own action and the biased\naction information of other agents in the two groups. The biased joint action\nof cooperative agents is computed as the sum of their actual joint action and\nthe imaginary cooperative joint action, by assuming all the cooperative agents\njointly maximize the target agent's value function. The biased joint action of\ncompetitive agents can be computed similarly. Each agent then updates its own\nvalue function using the biased action information, resulting in a biased value\nfunction and corresponding biased policy. Subsequently, the biased policy of\neach agent is inevitably subjected to recommend an action to cooperate and\ncompete with other agents, thereby introducing more active interactions among\nagents and enhancing the MARL policy learning. We empirically demonstrate that\nour algorithm outperforms existing algorithms in various mixed\ncooperative-competitive environments. Furthermore, the introduced biases\ngradually decrease as the training proceeds and the correction based on the\nimaginary assumption vanishes.",
    "pdf_url": "http://arxiv.org/pdf/2101.06890v1",
    "published": "2021-01-18"
  }
}